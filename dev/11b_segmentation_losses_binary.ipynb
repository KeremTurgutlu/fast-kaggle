{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions/blob/master/selim_sef/training/losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp segmentation.losses_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.58.dev0\n"
     ]
    }
   ],
   "source": [
    "import fastai; print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "cross_entropy = CrossEntropyFlat(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from local.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 3]), torch.Size([2, 1, 3, 3]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = torch.ones((1,2,3,3))\n",
    "\n",
    "_input2 = tensor([[\n",
    "[\n",
    "    [1,1,1],\n",
    "    [1,0,0],\n",
    "    [1,0,0]\n",
    "],\n",
    "[\n",
    "    [0,0,0],\n",
    "    [0,1,1],\n",
    "    [0,1,1]\n",
    "]\n",
    "]]).float()*1e3\n",
    "\n",
    "_input3 = tensor([[\n",
    "[\n",
    "    [1,1,1],\n",
    "    [1,0,0],\n",
    "    [1,0,0]\n",
    "],\n",
    "[\n",
    "    [0,0,0],\n",
    "    [0,1,1],\n",
    "    [0,1,1]\n",
    "]\n",
    "]]).float()*-1e3\n",
    "\n",
    "_target = tensor([[[\n",
    "    [0,0,0],\n",
    "    [0,1,1],\n",
    "    [0,1,1]\n",
    "]]])\n",
    "\n",
    "_input = torch.cat([_input, _input])\n",
    "_input2 = torch.cat([_input2, _input2])\n",
    "_input3 = torch.cat([_input3, _input3])\n",
    "_target = torch.cat([_target, _target])\n",
    "\n",
    "_input.size(), _target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class _SingleSoftmaxOutput(nn.Module):\n",
    "    \"layer for [B,2,H,W] logits -> [B,1,H,W] probas\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input, output):\n",
    "        input = input.softmax(1)[:,1].unsqueeze(1).contiguous()\n",
    "        return input, output\n",
    "\n",
    "_single_softmax_layer = _SingleSoftmaxOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1000.)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `dice_loss_v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def _soft_dice_loss(outputs, targets, per_image=False):    \n",
    "    batch_size = outputs.size()[0]\n",
    "    eps = 1e-5\n",
    "    if not per_image: batch_size = 1\n",
    "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
    "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
    "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
    "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n",
    "    loss = (1 - (2 * intersection + eps) / union).mean()\n",
    "    return loss\n",
    "\n",
    "class _DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
    "        super().__init__()\n",
    "        self.size_average = size_average\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = _single_softmax_layer(input, target)\n",
    "        return _soft_dice_loss(input, target, per_image=self.per_image)\n",
    "\n",
    "\n",
    "dice_loss_v2 = _DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5294)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_loss_v2(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_loss_v2(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_loss_v2(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `stable_bce_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class _StableBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = _single_softmax_layer(input, target)\n",
    "        input = input.float().view(-1)\n",
    "        target = target.float().view(-1)\n",
    "        neg_abs = - input.abs()\n",
    "        # todo check correctness\n",
    "        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "        return loss.mean()\n",
    "    \n",
    "stable_bce_loss = _StableBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7519)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_bce_loss(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5243)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_bce_loss(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0377)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_bce_loss(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `jaccard_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def _jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
    "    batch_size = outputs.size()[0]\n",
    "    eps = 1e-3\n",
    "    if not per_image:\n",
    "        batch_size = 1\n",
    "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
    "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
    "    target_sum = torch.sum(dice_target, dim=1)\n",
    "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
    "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
    "    if non_empty:\n",
    "        assert per_image == True\n",
    "        non_empty_images = 0\n",
    "        sum_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            if target_sum[i] > min_pixels:\n",
    "                sum_loss += losses[i]\n",
    "                non_empty_images += 1\n",
    "        if non_empty_images == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return sum_loss / non_empty_images\n",
    "\n",
    "    return losses.mean()\n",
    "\n",
    "class _JaccardLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, per_image=False, \n",
    "                 non_empty=False, apply_sigmoid=False, min_pixels=5):\n",
    "        super().__init__()\n",
    "        self.size_average = size_average\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.per_image = per_image\n",
    "        self.non_empty = non_empty\n",
    "        self.apply_sigmoid = apply_sigmoid\n",
    "        self.min_pixels = min_pixels\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = _single_softmax_layer(input, target)\n",
    "        return _jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
    "    \n",
    "jaccard_loss = _JaccardLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6923)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_loss (_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_loss (_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_loss (_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `lovasz_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def _lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts.float() - gt_sorted.float().cumsum(0)\n",
    "    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def _lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = _mean(_lovasz_hinge_flat(*_flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                    for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss =_lovasz_hinge_flat(*_flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * (signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad =_lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.relu(errors_sorted), (grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "def _lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = _mean(_lovasz_sigmoid_flat(*_flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss =_lovasz_sigmoid_flat(*_flatten_binary_scores(probas, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _lovasz_sigmoid_flat(probas, labels):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "    \"\"\"\n",
    "    fg = labels.float()\n",
    "    errors = ((fg) - probas).abs()\n",
    "    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "    perm = perm.data\n",
    "    fg_sorted = fg[perm]\n",
    "    loss = torch.dot(errors_sorted, (_lovasz_grad(fg_sorted)))\n",
    "    return loss\n",
    "\n",
    "def _symmetric_lovasz(outputs, targets, ):\n",
    "    return (_lovasz_hinge(outputs, targets) +_lovasz_hinge(-outputs, 1 - targets)) / 2\n",
    "\n",
    "def _mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "\n",
    "class _LovaszLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=0, per_image=True):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = _single_softmax_layer(input, target)\n",
    "        input = input.contiguous()\n",
    "        target = target.contiguous()\n",
    "        return _symmetric_lovasz(input, target)\n",
    "    \n",
    "lovasz_loss = _LovaszLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2778)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_loss(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7778)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_loss(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7778)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_loss(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `lovasz_loss_sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class _LovaszLossSigmoid(nn.Module):\n",
    "    def __init__(self, ignore_index=0, per_image=True):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = _single_softmax_layer(input, target)\n",
    "        input = input.contiguous()\n",
    "        target = target.contiguous()\n",
    "        return _lovasz_sigmoid(input, target, per_image=self.per_image, ignore=self.ignore_index)\n",
    "\n",
    "lovasz_loss_sigmoid = _LovaszLossSigmoid()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_loss_sigmoid(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_loss_sigmoid(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_loss_sigmoid(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `focal_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class _FocalLoss2d(nn.Module):\n",
    "    def __init__(self, gamma=2, ignore_index=255):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = _single_softmax_layer(input, target)\n",
    "        input = input.contiguous()\n",
    "        target = target.contiguous()\n",
    "        eps = 1e-8\n",
    "        non_ignored = target.view(-1) != self.ignore_index\n",
    "        target = target.view(-1)[non_ignored].float()\n",
    "        input = input.contiguous().view(-1)[non_ignored]\n",
    "        input = torch.clamp(input, eps, 1. - eps)\n",
    "        target = torch.clamp(target, eps, 1. - eps)\n",
    "        pt = (1 - target) * (1 - input) + target * input\n",
    "        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()\n",
    "    \n",
    "focal_loss = _FocalLoss2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1733)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.4207)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `combo_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class _ComboLoss(nn.Module):\n",
    "    def __init__(self, weights, per_image=False):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "        self.bce = _StableBCELoss()\n",
    "        self.dice = _DiceLoss(per_image=False)\n",
    "        self.jaccard = _JaccardLoss(per_image=False)\n",
    "        self.lovasz = _LovaszLoss(per_image=per_image)\n",
    "        self.lovasz_sigmoid = _LovaszLossSigmoid(per_image=per_image)\n",
    "        self.focal = _FocalLoss2d()\n",
    "        self.losses = {'bce': self.bce,\n",
    "                        'dice': self.dice,\n",
    "                        'focal': self.focal,\n",
    "                        'jaccard': self.jaccard,\n",
    "                        'lovasz': self.lovasz,\n",
    "                        'lovasz_sigmoid': self.lovasz_sigmoid}\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = 0\n",
    "        for k, w in self.weights.items(): loss += w * self.losses[k](input, target)\n",
    "        return loss.clamp(min=1e-5)\n",
    "\n",
    "_weights = {'bce': 1, 'dice': 1, 'focal': 1, 'jaccard': 1, 'lovasz': 1, 'lovasz_sigmoid': 1}\n",
    "combo_loss = _ComboLoss(_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9246)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_loss(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3021)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_loss(_input2, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.2361)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_loss(_input3, _target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_script.ipynb.\n",
      "Converted 02_scheduler.ipynb.\n",
      "Converted 03_callbacks.ipynb.\n",
      "Converted 10_segmentation_dataset.ipynb.\n",
      "Converted 11_segmentation_losses_mulitlabel.ipynb.\n",
      "Converted 11b_segmentation_losses_binary.ipynb.\n",
      "Converted 12_segmentation_metrics.ipynb.\n",
      "Converted 13_segmentation_models.ipynb.\n",
      "Converted segmentation_training.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
