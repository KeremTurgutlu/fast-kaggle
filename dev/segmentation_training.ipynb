{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script - do not run these cells \n",
    "\n",
    "relative imports fail when run as a script so scripts stays above library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.script import *\n",
    "from fastai.utils.mem import *\n",
    "\n",
    "from local.segmentation.dataset import *\n",
    "from local.segmentation import metrics\n",
    "from local.segmentation import losses\n",
    "from local.callbacks import *\n",
    "from local.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keeps this in cell index position: 2\n",
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.script import *\n",
    "from fastai.utils.mem import *\n",
    "\n",
    "from local.segmentation.dataset import *\n",
    "from local.segmentation import metrics\n",
    "from local.segmentation import losses\n",
    "from local.callbacks import *\n",
    "from local.optimizers import *\n",
    "\n",
    "# https://stackoverflow.com/questions/8299270/ultimate-answer-to-relative-python-imports\n",
    "@call_parse\n",
    "def main(    \n",
    "    PATH:Param(\"Path which have data\", str)=\"\",\n",
    "    IMAGES:Param(\"images folder path name\", str)=\"images\",\n",
    "    MASKS:Param(\"mask folder path name\", str)=\"masks\",\n",
    "    CODES:Param(\"codes.txt with pixel codes\", str)=\"\",\n",
    "    TRAIN:Param(\"train.txt with training image names\", str)=\"\",\n",
    "    VALID:Param(\"valid.txt with validation image names\", str)=None,\n",
    "    TEST:Param(\"test.txt with test image names\", str)=None,\n",
    "    suffix:Param(\"suffix for label filenames\", str)=\".png\",\n",
    "    sample_size:Param(\"\", int)=None,\n",
    "    bs:Param(\"Batch size\", int)=80,\n",
    "    size:Param(\"Image size\", int)=224,\n",
    "    imagenet_pretrained:Param(\"Use imagenet weights for DynamicUnet\", int)=1,\n",
    "    max_lr:Param(\"Learning Rate\", float)=3e-3,\n",
    "    model_name:Param(\"Model name for save\", str)=\"mybestmodel\",\n",
    "    epochs:Param(\"Number of max epochs to train\", int)=10,\n",
    "    tracking_metric:Param(\"Which metric to use for tracking and evaluation\", str)=\"dice\",\n",
    "    void_name:Param(\"Background class name\", str)=None,\n",
    "    loss_function:Param(\"Loss function for training\", str)=\"crossentropy\",\n",
    "    opt:Param(\"Optimizer for training\", str)=None,\n",
    "    arch_name:Param(\"Architecture backbone for training\", str)=\"resnet34\",\n",
    "    EXPORT_PATH:Param(\"Where to export trained model\", str)=\".\",\n",
    "    \n",
    "    gpu:Param(\"GPU to run on, can handle multi gpu\", str)=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    For Multi GPU Run: python ../fastai/fastai/launch.py {--gpus=0123} ./training.py {--your args}\n",
    "    For Single GPU Run: python ./training.py {--your args}\n",
    "    bs: 80 size: 224 , bs: 320 size: 112 \n",
    "    \"\"\"\n",
    "        \n",
    "    # Setup init\n",
    "    gpu = setup_distrib(gpu)\n",
    "    \n",
    "    # Args\n",
    "    if not gpu: print(f\"Print args here: \")\n",
    "        \n",
    "    # Get data\n",
    "    PATH = Path(PATH)\n",
    "    try: VALID = float(VALID)\n",
    "    except: pass\n",
    "    ssdata = SemanticSegmentationData(PATH, IMAGES, MASKS, CODES, TRAIN,\n",
    "                                      VALID, TEST, sample_size, bs, size, suffix)\n",
    "    data = ssdata.get_data()\n",
    "    if imagenet_pretrained: data.normalize(imagenet_stats)\n",
    "    else: data.normalize()   \n",
    "    \n",
    "    # learn - models: 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50',\n",
    "    arch = getattr(models, arch_name)\n",
    "    if not gpu: print(f\"Training with arch: {arch}\")\n",
    "    learn = unet_learner(data, arch = arch, pretrained = True)\n",
    "    learn.path, learn.model_dir = Path(EXPORT_PATH), 'models'\n",
    "\n",
    "    # metric\n",
    "    metric = getattr(metrics, tracking_metric)\n",
    "    if not gpu: print(f\"Tracking metric: {metric}\")\n",
    "    if tracking_metric in [\"multilabel_dice\", \"multilabel_iou\"]: metric = partial(metric, c=learn.data.c)\n",
    "    if tracking_metric == \"foreground_acc\": \n",
    "        void_code = np.where(learn.data.classes == void_name)[0].item()\n",
    "        metric = partial(metric, void_code=void_code)\n",
    "    learn.metrics = [metric]\n",
    "    \n",
    "    # loss\n",
    "    loss = getattr(losses, loss_function, None)\n",
    "    if loss: learn.loss_func = loss \n",
    "    if not gpu: print(f\"Training with loss: {learn.loss_func}\")\n",
    "\n",
    "    # callbacks\n",
    "    save_cb = SaveDistributedModelCallback(learn, tracking_metric, \"max\", name=model_name, gpu=gpu)\n",
    "    csvlog_cb = CSVDistributedLogger(learn, 'training_log', append=True, gpu=gpu)\n",
    "    cbs = [save_cb, csvlog_cb]\n",
    "        \n",
    "    # optimizer / scheduler\n",
    "    alpha=0.99; mom=0.9; eps=1e-8\n",
    "    \n",
    "    if   opt=='adam':        opt_func = partial(optim.Adam, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='radam':       opt_func = partial(RAdam, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='novograd':    opt_func = partial(Novograd, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='rms':         opt_func = partial(optim.RMSprop, alpha=alpha, eps=eps)\n",
    "    elif opt=='sgd':         opt_func = partial(optim.SGD, momentum=mom)\n",
    "    elif opt=='ranger':      opt_func = partial(Ranger,  betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='ralamb':      opt_func = partial(Ralamb,  betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='rangerlars':  opt_func = partial(RangerLars,  betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='lookahead':   opt_func = partial(LookaheadAdam, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='lamb':        opt_func = partial(Lamb, betas=(mom,alpha), eps=eps)\n",
    "    \n",
    "    if opt: learn.opt_func = opt_func\n",
    "\n",
    "    # distributed\n",
    "    if (gpu is not None) & (num_distrib()>1): learn.to_distributed(gpu)\n",
    "    \n",
    "    # to_fp16 \n",
    "    learn.to_fp16()\n",
    "    \n",
    "    # train\n",
    "    if not gpu: print(f\"Starting training with max_lr: {max_lr}\")\n",
    "    if imagenet_pretrained:\n",
    "        if not gpu: print(\"Training with transfer learning\")\n",
    "        # stage-1\n",
    "        learn.freeze_to(-1)\n",
    "        learn.fit_one_cycle(epochs, max_lr, callbacks=cbs)\n",
    "\n",
    "        # stage-2\n",
    "        lrs = slice(max_lr/100,max_lr/4)\n",
    "        learn.freeze_to(-2)\n",
    "        learn.fit_one_cycle(epochs, lrs, pct_start=0.8, callbacks=cbs)\n",
    " \n",
    "        # stage-3\n",
    "        lrs = slice(max_lr/100,max_lr/4)\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(epochs, lrs, pct_start=0.8, callbacks=cbs)\n",
    "    else:\n",
    "        if not gpu: print(\"Training from scratch\")\n",
    "        learn.fit_one_cycle(epochs, max_lr, callbacks=cbs)\n",
    "        \n",
    "    # save valid and test preds \n",
    "    if TEST: dtypes = [\"Valid\", \"Test\"]\n",
    "    else: dtypes = [\"Valid\"]\n",
    "    for dtype in dtypes:\n",
    "        if not gpu: print(f\"Generating Raw Predictions for {dtype}...\")\n",
    "        ds_type = getattr(DatasetType, dtype)\n",
    "        preds, targs = learn.get_preds(ds_type)\n",
    "        ds = learn.data.test_ds if dtype == \"Test\" else learn.data.valid_ds\n",
    "        fnames = list(ds.items)\n",
    "        try_save({\"fnames\":fnames, \"preds\":to_cpu(preds), \"targs\":to_cpu(targs)},\n",
    "                 path=Path(EXPORT_PATH), file=f\"{dtype}_raw_preds.pkl\")\n",
    "        if not gpu: print(f\"Done.\")\n",
    "            \n",
    "#     # to_fp32 + export learn\n",
    "#     if not gpu:\n",
    "#         learn.to_fp32()    \n",
    "#         learn.load(model_name) # load best saved model\n",
    "#         print(f\"Exporting model to: {EXPORT_PATH}\")\n",
    "#         learn.export(f\"{model_name}_export.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.notebook.export import *\n",
    "# xport script\n",
    "cells = read_nb(\"segmentation_training.ipynb\")['cells']\n",
    "src = cells[2]['source']\n",
    "with open(\"segmentation_training.py\", \"w\") as f: f.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `segmentation_training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from local.script import run_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n",
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n",
      "Print args here:\n",
      "Training with arch: <function resnet34 at 0x7fcf5510a2f0>\n",
      "Tracking metric: <function foreground_acc at 0x7fce5d7f4840>\n",
      "Training with loss: FlattenedLoss of CrossEntropyLoss()\n",
      "Starting training with max_lr: 0.003\n",
      "Training with transfer learning\n",
      "Initializing self.best\n",
      "epoch     train_loss  valid_loss  foreground_acc  time\n",
      "Initializing self.best\n",
      "0         14.694790   1.292422    0.654179        00:12\n",
      "Better model found at epoch 0 with foreground_acc value: 0.654179036617279.\n",
      "Total time: 00:12\n",
      "Total time: 00:13\n",
      "epoch     train_loss  valid_loss  foreground_acc  time\n",
      "0         1.221355    0.950214    0.753440        00:10\n",
      "Total time: 00:11\n",
      "Better model found at epoch 0 with foreground_acc value: 0.7534400820732117.\n",
      "Total time: 00:12\n",
      "epoch     train_loss  valid_loss  foreground_acc  time\n",
      "0         0.973882    0.763184    0.805589        00:11\n",
      "Better model found at epoch 0 with foreground_acc value: 0.8055894374847412.\n",
      "Total time: 00:12\n",
      "Generating Raw Predictions for Valid...\n",
      "Total time: 00:12\n",
      "Generating Raw Predictions for Valid...\n",
      "Generating Raw Predictions for Test...\n",
      "Done.\n",
      "Generating Raw Predictions for Test...\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc = run_command(f\"\"\"\n",
    "python {Path(fastai.__file__).parent}/launch.py \n",
    "--gpus=01 segmentation_training.py \\\n",
    "--PATH=/home/turgutluk/.fastai/data/camvid \\\n",
    "--IMAGES=images \\\n",
    "--MASKS=labels \\\n",
    "--CODES=codes.txt \\\n",
    "--TRAIN=train.txt \\\n",
    "--VALID=0.2 \\\n",
    "--TEST=test.txt \\\n",
    "--bs=4 \\\n",
    "--size=112 \\\n",
    "--imagenet_pretrained=1 \\\n",
    "--max_lr=3e-3 \\\n",
    "--model_name=mybestmodel \\\n",
    "--epochs=1 \\\n",
    "--tracking_metric=foreground_acc \\\n",
    "--void_name=Void \\\n",
    "--loss_function=xentropy \\\n",
    "--opt=radam\n",
    "--EXPORT_PATH=./experiment_export\n",
    "\"\"\", logfn=\"./experiment_export/stdouterr.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_PATH = Path(\"./experiment_export/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = json.loads(open(EXPORT_PATH/\"stdouterr.log\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': '09/21/2019, 16:23:14',\n",
       " 'command': ['python',\n",
       "  '/home/turgutluk/fastai/fastai/launch.py',\n",
       "  '--gpus=01',\n",
       "  'segmentation_training.py',\n",
       "  '--PATH=/home/turgutluk/.fastai/data/camvid',\n",
       "  '--IMAGES=images',\n",
       "  '--MASKS=labels',\n",
       "  '--CODES=codes.txt',\n",
       "  '--TRAIN=train.txt',\n",
       "  '--VALID=0.2',\n",
       "  '--TEST=test.txt',\n",
       "  '--bs=4',\n",
       "  '--size=112',\n",
       "  '--imagenet_pretrained=1',\n",
       "  '--max_lr=3e-3',\n",
       "  '--model_name=mybestmodel',\n",
       "  '--epochs=1',\n",
       "  '--tracking_metric=foreground_acc',\n",
       "  '--void_name=Void',\n",
       "  '--loss_function=xentropy',\n",
       "  '--opt=radam',\n",
       "  '--EXPORT_PATH=./experiment_export'],\n",
       " 'stderr': '',\n",
       " 'stdout': \"To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\\nTo use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\\nPrint args here: \\nTraining with arch: <function resnet34 at 0x7fcf5510a2f0>\\nTracking metric: <function foreground_acc at 0x7fce5d7f4840>\\nTraining with loss: FlattenedLoss of CrossEntropyLoss()\\nStarting training with max_lr: 0.003\\nTraining with transfer learning\\nInitializing self.best\\nepoch     train_loss  valid_loss  foreground_acc  time    \\nInitializing self.best\\n0         14.694790   1.292422    0.654179        00:12     \\nBetter model found at epoch 0 with foreground_acc value: 0.654179036617279.\\nTotal time: 00:12\\nTotal time: 00:13\\nepoch     train_loss  valid_loss  foreground_acc  time    \\n0         1.221355    0.950214    0.753440        00:10     \\nTotal time: 00:11\\nBetter model found at epoch 0 with foreground_acc value: 0.7534400820732117.\\nTotal time: 00:12\\nepoch     train_loss  valid_loss  foreground_acc  time    \\n0         0.973882    0.763184    0.805589        00:11     \\nBetter model found at epoch 0 with foreground_acc value: 0.8055894374847412.\\nTotal time: 00:12\\nGenerating Raw Predictions for Valid...\\nTotal time: 00:12\\nGenerating Raw Predictions for Valid...\\nGenerating Raw Predictions for Test...\\nDone.\\nGenerating Raw Predictions for Test...\\nDone.\\n\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('experiment_export/training_log.csv'),\n",
       " PosixPath('experiment_export/tmp'),\n",
       " PosixPath('experiment_export/Test_raw_preds.pkl'),\n",
       " PosixPath('experiment_export/Valid_raw_preds.pkl'),\n",
       " PosixPath('experiment_export/models'),\n",
       " PosixPath('experiment_export/mybestmodel_export.pkl'),\n",
       " PosixPath('experiment_export/stdouterr.log')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_PATH.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = torch.load(EXPORT_PATH/'Valid_raw_preds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4,  4,  4,  ...,  4,  4,  4],\n",
       "          [ 4,  4,  4,  ...,  4,  4,  4],\n",
       "          [ 4,  4,  4,  ..., 30,  4,  4],\n",
       "          ...,\n",
       "          [19, 19, 19,  ..., 17, 17, 17],\n",
       "          [19, 19, 19,  ..., 17, 17, 17],\n",
       "          [19, 19, 19,  ..., 17, 17, 17]]],\n",
       "\n",
       "\n",
       "        [[[26, 26, 26,  ...,  4,  4,  4],\n",
       "          [26, 26, 26,  ...,  4,  4,  4],\n",
       "          [26, 26, 26,  ...,  4,  4,  4],\n",
       "          ...,\n",
       "          [19, 19, 19,  ..., 30, 30, 30],\n",
       "          [19, 19, 19,  ..., 30, 30, 30],\n",
       "          [19, 19, 19,  ..., 30, 30, 30]]],\n",
       "\n",
       "\n",
       "        [[[ 4,  4,  4,  ...,  4,  4,  4],\n",
       "          [ 4,  4,  4,  ...,  4,  4,  4],\n",
       "          [ 4,  4,  4,  ...,  4,  4,  4],\n",
       "          ...,\n",
       "          [10, 17, 17,  ..., 17, 17, 17],\n",
       "          [17, 17, 17,  ..., 17, 17, 17],\n",
       "          [17, 17, 17,  ..., 17, 17, 17]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[21, 21, 21,  ...,  4,  4,  4],\n",
       "          [21, 21, 21,  ...,  4,  4,  4],\n",
       "          [21, 21, 21,  ...,  4,  4,  4],\n",
       "          ...,\n",
       "          [17, 17, 17,  ..., 17, 17, 17],\n",
       "          [17, 17, 17,  ..., 17, 17, 17],\n",
       "          [17, 17, 17,  ..., 17, 17, 17]]],\n",
       "\n",
       "\n",
       "        [[[26, 26, 26,  ..., 26, 26, 26],\n",
       "          [26, 26, 26,  ..., 26, 26, 26],\n",
       "          [26, 26, 26,  ..., 26, 26, 26],\n",
       "          ...,\n",
       "          [17, 17, 17,  ..., 30, 30, 30],\n",
       "          [17, 17, 17,  ..., 30, 30, 30],\n",
       "          [17, 17, 17,  ..., 30, 30, 30]]],\n",
       "\n",
       "\n",
       "        [[[ 4,  4,  4,  ..., 26, 26, 26],\n",
       "          [ 4,  4,  4,  ..., 26, 26, 26],\n",
       "          [ 4,  4,  4,  ..., 26, 26, 26],\n",
       "          ...,\n",
       "          [19, 19, 19,  ..., 17, 17, 17],\n",
       "          [19, 19, 19,  ..., 17, 17, 17],\n",
       "          [19, 19, 17,  ..., 17, 17, 17]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds['targs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
