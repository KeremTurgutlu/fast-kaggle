{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script - do not run these cells \n",
    "\n",
    "relative imports fail when run as a script so scripts stays above library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'local.distributed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66c684703876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'local.distributed'"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.script import *\n",
    "from fastai.utils.mem import *\n",
    "\n",
    "from local.segmentation.dataset import *\n",
    "from local.segmentation import metrics\n",
    "from local.segmentation import losses\n",
    "from local.distributed import *\n",
    "from local.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keeps this in cell index position: 2\n",
    "from fastai.vision import *\n",
    "from fastai.distributed import *\n",
    "from fastai.script import *\n",
    "from fastai.utils.mem import *\n",
    "\n",
    "from local.segmentation.dataset import *\n",
    "from local.segmentation import metrics\n",
    "from local.segmentation import losses\n",
    "from local.callbacks import *\n",
    "from local.optimizers import *\n",
    "\n",
    "# https://stackoverflow.com/questions/8299270/ultimate-answer-to-relative-python-imports\n",
    "@call_parse\n",
    "def main(    \n",
    "    PATH:Param(\"Path which have data\", str)=\"\",\n",
    "    IMAGES:Param(\"images folder path name\", str)=\"images\",\n",
    "    MASKS:Param(\"mask folder path name\", str)=\"masks\",\n",
    "    CODES:Param(\"codes.txt with pixel codes\", str)=\"\",\n",
    "    TRAIN:Param(\"train.txt with training image names\", str)=\"\",\n",
    "    VALID:Param(\"valid.txt with validation image names\", str)=None,\n",
    "    TEST:Param(\"test.txt with test image names\", str)=None,\n",
    "    sample_size:Param(\"\", int)=None,\n",
    "    bs:Param(\"Batch size\", int)=80,\n",
    "    size:Param(\"Image size\", int)=224,\n",
    "    imagenet_pretrained:Param(\"Use imagenet weights for DynamicUnet\", int)=1,\n",
    "    max_lr:Param(\"Learning Rate\", float)=3e-3,\n",
    "    model_name:Param(\"Model name for save\", str)=\"mybestmodel\",\n",
    "    epochs:Param(\"Number of max epochs to train\", int)=10,\n",
    "    tracking_metric:Param(\"Which metric to use for tracking and evaluation\", str)=\"dice\",\n",
    "    void_name:Param(\"Background class name\", str)=None,\n",
    "    loss_function:Param(\"Loss function for training\", str)=\"crossentropy\",\n",
    "    opt:Param(\"Optimizer for training\", str)=None,\n",
    "    arch_name:Param(\"Architecture backbone for training\", str)=\"resnet34\",\n",
    "    EXPORT_PATH:Param(\"Where to export trained model\", str)=\".\",\n",
    "    \n",
    "    gpu:Param(\"GPU to run on, can handle multi gpu\", str)=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    For Multi GPU Run: python ../fastai/fastai/launch.py {--gpus=0123} ./training.py {--your args}\n",
    "    For Single GPU Run: python ./training.py {--your args}\n",
    "    bs: 80 size: 224 , bs: 320 size: 112 \n",
    "    \"\"\"\n",
    "        \n",
    "    # Setup init\n",
    "    gpu = setup_distrib(gpu)\n",
    "    \n",
    "    # Args\n",
    "    if not gpu: print(f\"Print args here: \")\n",
    "        \n",
    "    # Get data\n",
    "    PATH = Path(PATH)\n",
    "    try: VALID = float(VALID)\n",
    "    except: pass\n",
    "    ssdata = SemanticSegmentationData(PATH, IMAGES, MASKS, CODES, TRAIN, VALID, TEST, sample_size, bs, size)\n",
    "    data = ssdata.get_data()\n",
    "    if imagenet_pretrained: data.normalize(imagenet_stats)\n",
    "    else: data.normalize()   \n",
    "    \n",
    "    # learn - models: 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50',\n",
    "    arch = getattr(models, arch_name)\n",
    "    if not gpu: print(f\"Training with arch: {arch}\")\n",
    "    learn = unet_learner(data, arch = arch, pretrained = True)\n",
    "    learn.path, learn.model_dir = Path(EXPORT_PATH), 'models'\n",
    "\n",
    "    # metric\n",
    "    metric = getattr(metrics, tracking_metric)\n",
    "    if not gpu: print(f\"Tracking metric: {metric}\")\n",
    "    if tracking_metric in [\"multilabel_dice\", \"multilabel_iou\"]: metric = partial(metric, c=learn.data.c)\n",
    "    if tracking_metric == \"foreground_acc\": \n",
    "        void_code = np.where(learn.data.classes == void_name)[0].item()\n",
    "        metric = partial(metric, void_code=void_code)\n",
    "    learn.metrics = [metric]\n",
    "    \n",
    "    # loss\n",
    "    loss = getattr(losses, loss_function, None)\n",
    "    if loss: learn.loss_func = loss \n",
    "    if not gpu: print(f\"Training with loss: {learn.loss_func}\")\n",
    "\n",
    "    # callbacks\n",
    "    save_cb = SaveDistributedModelCallback(learn, tracking_metric, \"max\", name=model_name, gpu=gpu)\n",
    "    csvlog_cb = CSVLogger(learn, 'training_log', append=True)\n",
    "    cbs = [save_cb, csvlog_cb]\n",
    "        \n",
    "    # optimizer / scheduler\n",
    "    alpha=0.99; mom=0.9; eps=1e-8\n",
    "    \n",
    "    if   opt=='adam' : opt_func = partial(optim.Adam, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='radam' : opt_func = partial(RAdam, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='novograd' : opt_func = partial(Novograd, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='rms'  : opt_func = partial(optim.RMSprop, alpha=alpha, eps=eps)\n",
    "    elif opt=='sgd'  : opt_func = partial(optim.SGD, momentum=mom)\n",
    "    elif opt=='ranger'  : opt_func = partial(Ranger,  betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='ralamb'  : opt_func = partial(Ralamb,  betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='rangerlars'  : opt_func = partial(RangerLars,  betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='lookahead'  : opt_func = partial(LookaheadAdam, betas=(mom,alpha), eps=eps)\n",
    "    elif opt=='lamb'  : opt_func = partial(Lamb, betas=(mom,alpha), eps=eps)\n",
    "    if opt: learn.opt_func = opt_func\n",
    "\n",
    "    # distributed\n",
    "    if (gpu is not None) & (num_distrib()>1): learn.to_distributed(gpu)\n",
    "    \n",
    "    # to_fp16 \n",
    "    learn.to_fp16()\n",
    "    \n",
    "    # train\n",
    "    if not gpu: print(f\"Starting training with max_lr: {max_lr}\")\n",
    "    if imagenet_pretrained:\n",
    "        if not gpu: print(\"Training with transfer learning\")\n",
    "        # stage-1\n",
    "        learn.freeze_to(-1)\n",
    "        learn.fit_one_cycle(epochs, max_lr, callbacks=cbs)\n",
    "\n",
    "        # stage-2\n",
    "        lrs = slice(max_lr/100,max_lr/4)\n",
    "        learn.freeze_to(-2)\n",
    "        learn.fit_one_cycle(epochs, lrs, pct_start=0.8, callbacks=cbs)\n",
    " \n",
    "        # stage-3\n",
    "        lrs = slice(max_lr/100,max_lr/4)\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(epochs, lrs, pct_start=0.8, callbacks=cbs)\n",
    "    else:\n",
    "        if not gpu: print(\"Training from scratch\")\n",
    "        learn.fit_one_cycle(epochs, max_lr, callbacks=cbs)\n",
    "        \n",
    "    # save valid and test preds \n",
    "    if TEST: dtypes = [\"Valid\", \"Test\"]\n",
    "    else: dtypes = [\"Valid\"]\n",
    "    for dtype in dtypes:\n",
    "        if not gpu: print(f\"Generating Raw Predictions for {dtype}...\")\n",
    "        preds, targs = learn.get_preds(getattr(DatasetType, dtype))\n",
    "        fnames = list(data.test_ds.items)\n",
    "        try_save({\"fnames\":fnames, \"preds\":to_cpu(preds), \"targs\":to_cpu(targs)},\n",
    "                 path=Path(EXPORT_PATH), file=f\"{dtype}_raw_preds.pkl\")\n",
    "\n",
    "    # to_fp32 + export learn\n",
    "    learn.to_fp32()    \n",
    "    learn.load(model_name) # load best saved model\n",
    "    if not gpu: print(f\"Exporting model to: {EXPORT_PATH}\")\n",
    "    learn.export(f\"{model_name}_export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.notebook.export import *\n",
    "# export script\n",
    "cells = read_nb(\"segmentation_training.ipynb\")['cells']\n",
    "src = cells[2]['source']\n",
    "with open(\"segmentation_training.py\", \"w\") as f: f.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `run_command`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from local.script import run_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 452\n",
      "-rw-rw-r--. 1 turgutluk turgutluk   2168 Sep 13 17:40 00_testing_notebook_export.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk 135347 Sep 13 22:47 00_test.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk   2870 Sep 15 20:40 01_script.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk  58279 Sep 15 19:59 02_scheduler.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk  32124 Sep 15 20:30 03_callbacks.ipynb\n",
      "-rw-r--r--. 1 turgutluk turgutluk 158973 Sep 13 22:05 10_segmentation_dataset.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk   2657 Sep 13 22:27 11_segmentation_losses.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk   5016 Sep 14 16:48 12_segmentation_metrics.ipynb\n",
      "drwxrwxr-x. 4 turgutluk turgutluk   4096 Sep 15 20:38 experiment_export\n",
      "drwxrwxr-x. 2 turgutluk turgutluk   4096 Sep 13 22:46 images\n",
      "-rw-rw-r--. 1 turgutluk turgutluk    895 Sep 13 17:38 lib.pkl\n",
      "drwxrwxr-x. 7 turgutluk turgutluk   4096 Sep 15 20:25 local\n",
      "drwxrwxr-x. 2 turgutluk turgutluk   4096 Sep 15 20:28 models\n",
      "-rw-r--r--. 1 turgutluk turgutluk  15001 Sep 15 20:47 segmentation_training.ipynb\n",
      "-rw-rw-r--. 1 turgutluk turgutluk   5919 Sep 15 20:46 segmentation_training.py\n",
      "-rw-r--r--. 1 turgutluk turgutluk   1165 Sep 11 22:34 test_training.sh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(b'', b'')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_command([\"ls\", \"-l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n",
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n",
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n",
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n",
      "Print args here:\n",
      "Training with arch: <function resnet34 at 0x7f4b31a4b0d0>\n",
      "Tracking metric: <function foreground_acc at 0x7f4a3a131620>\n",
      "Training with loss: FlattenedLoss of CrossEntropyLoss()\n",
      "Starting training with max_lr: 0.003\n",
      "Training with transfer learning\n",
      "Initializing self.best\n",
      "epoch     train_loss  valid_loss  foreground_acc  time\n",
      "Initializing self.best\n",
      "Initializing self.best\n",
      "Initializing self.best\n",
      "Total time: 00:11\n",
      "0         129.438187  2.394941    0.331523        00:11\n",
      "Better model found at epoch 0 with foreground_acc value: 0.3315233886241913.\n",
      "Total time: 00:11\n",
      "Total time: 00:11\n",
      "Total time: 00:12\n",
      "epoch     train_loss  valid_loss  foreground_acc  time\n",
      "0         1.933858    1.450020    0.624617        00:07\n",
      "Total time: 00:08\n",
      "Total time: 00:08\n",
      "Better model found at epoch 0 with foreground_acc value: 0.6246166229248047.\n",
      "Total time: 00:08\n",
      "Total time: 00:08\n",
      "epoch     train_loss  valid_loss  foreground_acc  time\n",
      "Total time: 00:08\n",
      "0         1.240627    1.201242    0.686862        00:07\n",
      "Better model found at epoch 0 with foreground_acc value: 0.686862051486969.\n",
      "Total time: 00:08\n",
      "Total time: 00:09\n",
      "Total time: 00:09\n",
      "Generating Raw Predictions for Valid...\n",
      "Generating Raw Predictions for Test...\n",
      "Exporting model to: ./experiment_export\n"
     ]
    }
   ],
   "source": [
    "stdout, stderr = run_command(f\"\"\"\n",
    "python {Path(fastai.__file__).parent}/launch.py \n",
    "--gpus=0123 segmentation_training.py \\\n",
    "--PATH=/home/turgutluk/.fastai/data/camvid \\\n",
    "--IMAGES=images \\\n",
    "--MASKS=labels \\\n",
    "--CODES=codes.txt \\\n",
    "--TRAIN=train.txt \\\n",
    "--VALID=0.2 \\\n",
    "--TEST=test.txt \\\n",
    "--bs=4 \\\n",
    "--size=112 \\\n",
    "--imagenet_pretrained=1 \\\n",
    "--max_lr=3e-3 \\\n",
    "--model_name=mybestmodel \\\n",
    "--epochs=1 \\\n",
    "--tracking_metric=foreground_acc \\\n",
    "--void_name=Void \\\n",
    "--loss_function=xentropy \\\n",
    "--opt=radam\n",
    "--EXPORT_PATH=./experiment_export\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
