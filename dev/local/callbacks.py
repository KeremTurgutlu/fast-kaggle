#AUTOGENERATED! DO NOT EDIT! File to edit: dev/03_callbacks.ipynb (unless otherwise specified).

__all__ = ['TerminateOnNaNCallback', 'SaveDistributedModelCallback', 'CSVDistributedLogger']

#Cell
from fastai.vision import *
from fastai.callbacks import *
from time import time
from fastprogress.fastprogress import format_time

#Cell
class TerminateOnNaNCallback(Callback):
    "A `Callback` that terminates training if loss is NaN."

    def __init__(self):
        self.stop = False
        self.isnan = False

    def on_batch_end(self, last_loss, epoch, num_batch, **kwargs:Any)->None:
        "Test if `last_loss` is NaN and interrupts training."
        if self.stop: return True #to skip validation after stopping during training
        if torch.isnan(last_loss):
            print (f'Epoch/Batch ({epoch}/{num_batch}): Invalid loss, terminating training.')
            self.isnan = True
            return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}

#Cell
class SaveDistributedModelCallback(TrackerCallback):
    "SaveModelCallback modified for distributed transfer learning - remove torch.load"
    def __init__(self, learn:Learner, monitor:str='val_loss', mode:str='auto', every:str='improvement',
                 name:str='bestmodel', gpu=None):
        super().__init__(learn, monitor=monitor, mode=mode)
        self.every,self.name = every,name
        if self.every not in ['improvement', 'epoch']:
            warn(f'SaveModel every {self.every} is invalid, falling back to "improvement".')
            self.every = 'improvement'
        self.gpu = gpu

    def on_train_begin(self, **kwargs:Any)->None:
        "Initializes the best value."
        if not hasattr(self, 'best'):
            print("Initializing self.best")
            self.best = float('inf') if self.operator == np.less else -float('inf')

    def jump_to_epoch(self, epoch:int)->None:
        try:
            self.learn.load(f'{self.name}_{epoch-1}', purge=False)
            print(f"Loaded {self.name}_{epoch-1}")
        except: print(f'Model {self.name}_{epoch-1} not found.')

    def on_epoch_end(self, epoch:int, **kwargs:Any)->None:
        "Compare the value monitored to its best score and maybe save the model."
        if self.every=="epoch": self.learn.save(f'{self.name}_{epoch}')
        else:
            current = self.get_monitor_value()
            if current is not None and self.operator(current, self.best):
                if not self.gpu: print(f'Better model found at epoch {epoch} with {self.monitor} value: {current}.')
                self.best = current
                self.learn.save(f'{self.name}')

#Cell
class CSVDistributedLogger(LearnerCallback):
    "CSVLogger modified for distributed transfer learning - only write with gpu:0"
    def __init__(self, learn:Learner, filename: str = 'history', append: bool = False, gpu: int = None):
        super().__init__(learn)
        self.filename,self.path,self.append = filename,self.learn.path/f'{filename}.csv',append
        self.add_time = True
        self.gpu = gpu

    def read_logged_file(self):
        "Read the content of saved file"
        return pd.read_csv(self.path)

    def on_train_begin(self, **kwargs: Any) -> None:
        "Prepare file with metric names."
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.file = self.path.open('a') if self.append else self.path.open('w')
        if not self.gpu: self.file.write(','.join(self.learn.recorder.names[:(None if self.add_time else -1)]) + '\n')

    def on_epoch_begin(self, **kwargs:Any)->None:
        if self.add_time: self.start_epoch = time()

    def on_epoch_end(self, epoch: int, smooth_loss: Tensor, last_metrics: MetricsList, **kwargs: Any) -> bool:
        "Add a line with `epoch` number, `smooth_loss` and `last_metrics`."
        last_metrics = ifnone(last_metrics, [])
        stats = [str(stat) if isinstance(stat, int) else '#na#' if stat is None else f'{stat:.6f}'
                 for name, stat in zip(self.learn.recorder.names, [epoch, smooth_loss] + last_metrics)]
        if self.add_time: stats.append(format_time(time() - self.start_epoch))
        str_stats = ','.join(stats)
        if not self.gpu: self.file.write(str_stats + '\n')
        self.file.flush()
        os.fsync(self.file.fileno())

    def on_train_end(self, **kwargs: Any) -> None:
        "Close the file."
        self.file.close()

#Cell
def _get_metric_name(f):
    try: return f.func.__name__
    except: return f.__name__