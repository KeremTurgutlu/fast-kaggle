{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default fastai tta don't denorm prediction so works only for plain non spatial predictions:**\n",
    "\n",
    "https://github.com/fastai/fastai/blob/d82d1f8fbd67e83f21eb128005646977dda70db6/fastai/vision/tta.py#L19\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp segmentation.tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "1.0.59.dev0\n"
     ]
    }
   ],
   "source": [
    "%autosave 60 \n",
    "import fastai; print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _flip_ud(x):\n",
    "    \"Flip `x` vertically.\"\n",
    "    if isinstance(x, ImagePoints):\n",
    "        x.flow.flow[...,1] *= -1\n",
    "        return x\n",
    "    return tensor(np.ascontiguousarray(np.array(x)[:,::-1].copy()))\n",
    "flip_ud = TfmPixel(_flip_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros((1,256,256))\n",
    "t[:,256//2:, 256//2:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAYAAABccqhmAAAABHNCSVQICAgIfAhkiAAAAy5JREFUeJzt1kERgEAQA0GOQhoSMIsLVGADZOxjuhXkNZV1ruvbgKR9egAwRwAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAg7JgewKz7faYnMMgDgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgLAfzlgGCb2MbBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "Image (1, 256, 256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image(t)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAYAAABccqhmAAAABHNCSVQICAgIfAhkiAAAAy5JREFUeJzt1kERgEAQA0GOQhoSMIsLVGADZOxjuhXkNZV1ruvbgKR9egAwRwAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAg7JgewKz7faYnMMgDgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgLAfzlgGCb2MbBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "Image (1, 256, 256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image(t)\n",
    "Image(torch.flip(flip_lr(img).data, dims=[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAYAAABccqhmAAAABHNCSVQICAgIfAhkiAAAAy5JREFUeJzt1kERgEAQA0GOQhoSMIsLVGADZOxjuhXkNZV1ruvbgKR9egAwRwAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAgTAAg7JgewKz7faYnMMgDgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgDABgLAfzlgGCb2MbBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "Image (1, 256, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image(t)\n",
    "Image(torch.flip(flip_ud(img).data, dims=[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_train import _loss_func2activ\n",
    "def _seg_tta_only_v1(learn:Learner, ds_type:DatasetType=DatasetType.Valid) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for non-flip and flip_lr augmented inputs\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    try:\n",
    "        pbar = master_bar(range(2))\n",
    "        for i in pbar:\n",
    "            tfm = [] # to remove random crop resize aug\n",
    "            if i: tfm.append(flip_lr(p=1.))\n",
    "            ds.tfms = tfm\n",
    "            yield get_preds(learn.model, dl, pbar=pbar, activ=_loss_func2activ(learn.loss_func))[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.seg_tta_only_v1 = _seg_tta_only_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _seg_tta_only_v2(learn:Learner, ds_type:DatasetType=DatasetType.Valid) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for non-flip, flip_lr and flip_ud augmented inputs\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    try:\n",
    "        pbar = master_bar(range(3))\n",
    "        for i in pbar:\n",
    "            tfm = [] # to remove random crop resize aug\n",
    "            if i == 1: tfm.append(flip_lr(p=1.))\n",
    "            elif i == 2: tfm.append(flip_ud(p=1.))\n",
    "            ds.tfms = tfm\n",
    "            yield get_preds(learn.model, dl, pbar=pbar, activ=_loss_func2activ(learn.loss_func))[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "Learner.seg_tta_only_v2 = _seg_tta_only_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip_lr TTA preds\n",
    "def _seg_TTA(learn:Learner, ds_type=DatasetType.Valid, updown=False):\n",
    "    \"Takes average of original and flip_lr\"\n",
    "    if not updown:\n",
    "        orig_preds, flip_lr_preds = list(learn.seg_tta_only_v1(ds_type))\n",
    "        flip_lr_preds = torch.stack([torch.flip(o, dims=[-1]) for o in flip_lr_preds], dim=0)\n",
    "        avg_preds = (orig_preds + flip_lr_preds) / 2\n",
    "    else:\n",
    "        orig_preds, flip_lr_preds, flip_ud_preds = list(learn.seg_tta_only_v2(ds_type))        \n",
    "        flip_lr_preds = torch.stack([torch.flip(o, dims=[-1]) for o in flip_lr_preds], dim=0)\n",
    "        flip_ud_preds = torch.stack([torch.flip(o, dims=[-2]) for o in flip_ud_preds], dim=0)\n",
    "        avg_preds = (orig_preds + flip_lr_preds + flip_ud_preds) / 3\n",
    "    return avg_preds\n",
    "\n",
    "Learner.segTTA = _seg_TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.segmentation.dataset import SemanticSegmentationData\n",
    "from local.segmentation.metrics import *\n",
    "# from local.segmentation.losses_binary import *\n",
    "from local.segmentation.losses_multilabel import *\n",
    "# test data creation\n",
    "PATH = Path(\"/home/turgutluk/.fastai/data/camvid\")\n",
    "IMAGES = \"images\"\n",
    "MASKS = \"labels\"\n",
    "CODES = \"codes.txt\"\n",
    "TRAIN, VALID, TEST = \"train.txt\", \"valid.txt\", \"test.txt\"\n",
    "ssdata = SemanticSegmentationData(PATH, IMAGES, MASKS, CODES, TRAIN,\n",
    "                                  VALID, TEST, sample_size=None, bs=4, size=112)\n",
    "data = ssdata.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34); \n",
    "learn.metrics = [partial(foreground_acc, void_code=30)]\n",
    "learn.path = Path(\".\")\n",
    "learn.loss_func = lovasz_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 01:38 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>foreground_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.834825</td>\n",
       "      <td>0.760755</td>\n",
       "      <td>0.729466</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.724254</td>\n",
       "      <td>0.765455</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.727659</td>\n",
       "      <td>0.701049</td>\n",
       "      <td>0.768020</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685124</td>\n",
       "      <td>0.672644</td>\n",
       "      <td>0.790862</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.663146</td>\n",
       "      <td>0.649177</td>\n",
       "      <td>0.791330</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = learn.metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targs = learn.get_preds()\n",
    "score = metric(preds, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:05 <p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tta_v1_preds = learn.segTTA(updown=False)\n",
    "assert metric(tta_v1_preds, targs) > score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:08 <p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tta_v2_preds = learn.segTTA(updown=True)\n",
    "assert metric(tta_v2_preds, targs) > score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_script.ipynb.\n",
      "Converted 02_scheduler.ipynb.\n",
      "Converted 03_callbacks.ipynb.\n",
      "Converted 04_optimizers_optimizers.ipynb.\n",
      "Converted 10_segmentation_dataset.ipynb.\n",
      "Converted 11_segmentation_losses_mulitlabel.ipynb.\n",
      "Converted 11b_segmentation_losses_binary.ipynb.\n",
      "Converted 12_segmentation_metrics.ipynb.\n",
      "Converted 13_segmentation_models.ipynb.\n",
      "Converted 14_segmentation_postprocess.ipynb.\n",
      "Converted 15_segmentation_tta.ipynb.\n",
      "Converted 15_segmentation_utils.ipynb.\n",
      "Converted segmentation_training.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
