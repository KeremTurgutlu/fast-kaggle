{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp segmentation.losses_multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.58.dev0\n"
     ]
    }
   ],
   "source": [
    "import fastai; print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision import *\n",
    "from local.segmentation.lovasz_loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = [\"lovasz_softmax\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 3]), torch.Size([2, 1, 3, 3]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from local.test import *\n",
    "\n",
    "_softmax_input = torch.cat([torch.zeros((1,1,3,3)), torch.ones((1,1,3,3))],dim=1)\n",
    "\n",
    "_softmax_target = tensor([[[\n",
    "    [0,0,0],\n",
    "    [0,1,1],\n",
    "    [0,1,1]\n",
    "]]])\n",
    "\n",
    "_input.size(), _target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2689, 0.2689, 0.2689],\n",
       "          [0.2689, 0.2689, 0.2689],\n",
       "          [0.2689, 0.2689, 0.2689]],\n",
       "\n",
       "         [[0.7311, 0.7311, 0.7311],\n",
       "          [0.7311, 0.7311, 0.7311],\n",
       "          [0.7311, 0.7311, 0.7311]]]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_softmax_input.softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0],\n",
       "          [0, 1, 1],\n",
       "          [0, 1, 1]]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_softmax_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "cross_entropy = CrossEntropyFlat(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_close(cross_entropy(_softmax_input, _softmax_target), \n",
    "           -(np.log(0.2689)*5 + np.log(0.7311)*4)/9, eps=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lovasz_softmax` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6284)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random\n",
    "(lovasz_softmax(_softmax_input, _softmax_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bce_with_logits_flat_loss`\n",
    "\n",
    "Loss for **models that don't predict background** and use sigmoid + threshold instead of softmax/argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LabelBinarizer(Module):    \n",
    "    \"converts a 2d tensor target label to 3d binary representation\"\n",
    "    def __init__(self, exclude_zero=True):\n",
    "        self.exclude_zero = exclude_zero\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        c, dtype = input.size(1), input.dtype\n",
    "        trange = torch.arange(1 if self.exclude_zero else 0, c+1)\n",
    "        trange = trange[None,...,None,None].to(target.device)\n",
    "        return (target == trange).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 3]), torch.Size([2, 1, 3, 3]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sigmoid_input = torch.randn(2,4,3,3)\n",
    "# 0: void\n",
    "_sigmoid_target = tensor([\n",
    "    [[\n",
    "    [1,0,0],\n",
    "    [0,1,2],\n",
    "    [0,3,4]\n",
    "]],\n",
    "      [[\n",
    "    [1,0,0],\n",
    "    [0,1,2],\n",
    "    [0,3,4]\n",
    "]]\n",
    "\n",
    "])\n",
    "\n",
    "_input.size(), _target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "_binary_target = label_binarizer(_sigmoid_input, _sigmoid_target)\n",
    "assert _binary_target.shape == _input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _BCEWithLogitsFlatLoss(Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.label_binarizer = LabelBinarizer(**kwargs)\n",
    "        self.loss_fn = BCEWithLogitsFlat()\n",
    "    def forward(self, input, target):\n",
    "        return self.loss_fn(input, self.label_binarizer(input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "close:\n0.73676997423172\n0.83",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-4d8827d90b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_BCEWithLogitsFlatLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.83\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/fast-kaggle/dev/local/test.py\u001b[0m in \u001b[0;36mtest_close\u001b[0;34m(a, b, eps)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m\"`test` that `a` is within `eps` of `b`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_close\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fast-kaggle/dev/local/test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m\"`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: close:\n0.73676997423172\n0.83"
     ]
    }
   ],
   "source": [
    "l = _BCEWithLogitsFlatLoss()\n",
    "test_close(l(_input, _target), 0.83, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "bce_with_logits_flat_loss = _BCEWithLogitsFlatLoss(exclude_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_with_logits_flat_loss.label_binarizer.exclude_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BCEDiceLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/utils/losses.py\n",
    "\n",
    "def _f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pr (torch.Tensor): A list of predicted elements\n",
    "        gt (torch.Tensor):  A list of elements that are to be predicted\n",
    "        beta (float): positive constant\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: F score\n",
    "    \"\"\"\n",
    "\n",
    "    if activation is None or activation == \"none\":\n",
    "        activation_fn = lambda x: x\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_fn = torch.nn.Sigmoid()\n",
    "    elif activation == \"softmax2d\":\n",
    "        activation_fn = torch.nn.Softmax2d()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Activation implemented for sigmoid and softmax2d\"\n",
    "        )\n",
    "\n",
    "    pr = activation_fn(pr)\n",
    "\n",
    "    if threshold is not None:\n",
    "        pr = (pr > threshold).float()\n",
    "\n",
    "\n",
    "    tp = torch.sum(gt * pr)\n",
    "    fp = torch.sum(pr) - tp\n",
    "    fn = torch.sum(gt) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + eps) \\\n",
    "            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "class _DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-7, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.eps = eps\n",
    "        self.label_binarizer = LabelBinarizer()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = self.label_binarizer(input, target)\n",
    "        return 1 - _f_score(input, target, beta=1., eps=self.eps, \n",
    "                            threshold=None, activation=self.activation)\n",
    "    \n",
    "    \n",
    "class _BCEDiceLoss(_DiceLoss):\n",
    "    def __init__(self, eps=1e-7, activation='sigmoid'):\n",
    "        super().__init__(eps, activation)\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.label_binarizer = LabelBinarizer()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        dice = super().forward(input, target)\n",
    "        target = self.label_binarizer(input, target)\n",
    "        bce = self.bce(input, target)\n",
    "        \n",
    "        return dice + bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2 = _DiceLoss(), _BCEDiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "_l1, _l2 = l1(_input, _target), l2(_input, _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(_l2 - _l1 , 0.83, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fscore_dice_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "fscore_dice_loss  = _DiceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bce_dice_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "bce_dice_loss = _BCEDiceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_script.ipynb.\n",
      "Converted 02_scheduler.ipynb.\n",
      "Converted 03_callbacks.ipynb.\n",
      "Converted 10_segmentation_dataset.ipynb.\n",
      "Converted 11_segmentation_losses_mulitlabel.ipynb.\n",
      "Converted 11b_segmentation_losses_binary.ipynb.\n",
      "Converted 12_segmentation_metrics.ipynb.\n",
      "Converted 13_segmentation_models.ipynb.\n",
      "Converted segmentation_training.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
